# -*- coding: utf-8 -*-
"""FLAW.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1as8FpftkzIutE3QbjUpa6rfd_fmD_ggY

#IMPORTS
"""

#Keras and TensorFlow
import keras
from keras import layers
import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
#Progress Bars
import time
import progressbar
#Encoding
from tensorflow.keras.preprocessing.sequence import pad_sequences
#Tokenize
import collections
from collections import Counter
import itertools
from itertools import chain
import string
import nltk as nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
nltk.download('punkt')

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive

"""#Build Dataset

##Download Dataset
"""

ds, info = tfds.load("billsum", with_info=True)
ds_ca_test, ds_test, ds_train = ds["ca_test"], ds["test"], ds["train"] 
print(info)
print(ds_train)

ds_numpy = tfds.as_numpy(ds_train);

#for summ in ds_numpy:
 # print(summ)

###PRINTS EVERY SUMMARY IN THE DS
#for ins in ds_train:
 # summary_i = ins["summary"]
  #print(summary_i)



#FeaturesDict({
 #   'summary': Text(shape=(), dtype=tf.string),
  #  'text': Text(shape=(), dtype=tf.string),
   # 'title': Text(shape=(), dtype=tf.string),
#})

"""##Dataset

####Clean DS Code from DS creator
"""

import os
import pandas as pd
import pickle
import re
import unicodedata

def normalize_input(text):
  return unicodedata.normalize('NFD', text).encode('ascii', 'ignore')
  
def replace_semicolon(text, threshold=10):
    '''
    Get rid of semicolons.
    First split text into fragments between the semicolons. If the fragment 
    is longer than the threshold, turn the semicolon into a period. O.w treat
    it as a comma.
    Returns new text
    '''
    new_text = ""
    for subset in re.split(';', text):
        subset = subset.strip() # Clear off spaces
        # Check word count
        if len(subset.split()) > threshold:
            # Turn first char into uppercase
            new_text += ". " + subset[0].upper() + subset[1:]
        else:
            # Just append with a comma 
            new_text += ", " + subset

    return new_text

USC_re = re.compile('[Uu]\.*[Ss]\.*[Cc]\.]+')
PAREN_re = re.compile('\([^(]+\ [^\(]+\)')
BAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('"#%&\*\+/<=>@[\]^{|}~_'), re.UNICODE)
BULLET_RE = re.compile('\n[\ \t]*`*\([a-zA-Z0-9]*\)')
DASH_RE = re.compile('--+')
WHITESPACE_RE = re.compile('\s+')
EMPTY_SENT_RE = re.compile('[,\.]\ *[\.,]')
FIX_START_RE = re.compile('^[^A-Za-z]*')
FIX_PERIOD = re.compile('\.([A-Za-z])')
SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\.|\nSEC\.* [0-9]{1,2}\.|Sec\.* [0-9]{1,2}\.')

FIX_PERIOD = re.compile('\.([A-Za-z])')

SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\.|\nSEC\.* [0-9]{1,2}\.|Sec\.* [0-9]{1,2}\.')

def clean_text(text):
    """
    Borrowed from the FNDS text processing with additional logic added in.
    Note: we do not take care of token breaking - assume SPACY's tokenizer
    will handle this for us.
    """

    # Indicate section headers, we need them for features
    text = SECTION_HEADER_RE.sub('SECTION-HEADER', text)
    # For simplicity later, remove '.' from most common acronym
    text = text.replace("U.S.", "US")
    text = text.replace('SEC.', 'Section')
    text = text.replace('Sec.', 'Section')
    text = USC_re.sub('USC', text)

    # Remove parantheticals because they are almost always references to laws 
    # We could add a special tag, but we just remove for now
    # Note we dont get rid of nested parens because that is a complex re
    #text = PAREN_re.sub('LAWREF', text)
    text = PAREN_re.sub('', text)
    

    # Get rid of enums as bullets or ` as bullets
    text = BULLET_RE.sub(' ',text)
    
    # remove &nbsp
    text = text.replace('&nbsp', '')


    # Clean html 
    text = text.replace('&lt;all&gt;', '')

    # Remove annoying punctuation, that's not relevant
    text = BAD_PUNCT_RE.sub('', text)

    # Get rid of long sequences of dashes - these are formating
    text = DASH_RE.sub( ' ', text)

    # removing newlines, tabs, and extra spaces.
    text = WHITESPACE_RE.sub(' ', text)
    
    # If we ended up with "empty" sentences - get rid of them.
    text = EMPTY_SENT_RE.sub('.', text)
    
    # Attempt to create sentences from bullets 
    text = replace_semicolon(text)
    
    # Fix weird period issues + start of text weirdness
    #text = re.sub('\.(?=[A-Z])', '  . ', text)
    # Get rid of anything thats not a word from the start of the text
    text = FIX_START_RE.sub( '', text)
    # Sometimes periods get formatted weird, make sure there is a space between periods and start of sent   
    text = FIX_PERIOD.sub(". \g<1>", text)

    # Fix quotes
    text = text.replace('``', '"')
    text = text.replace('\'\'', '"')
    text = str(normalize_input(text))

    # Add special punct back in
    text = text.replace('SECTION-HEADER', '')
    text = text.replace('SHORT TITLE.', '')

    text = re.sub('Section [0-9]{1,2}\.', '', text)
    text = re.sub("(\w)quot(\s|\.|$)", r"\1 ", text)

    return text

"""###Tokenize"""

train_tokens_file = '/content/drive/My Drive/glove/train_tokens.pkl'
test_tokens_file =  '/content/drive/My Drive/glove/test_tokens.pkl'
import os.path
from os import path
#load object from file
def load_tok(datastruct_file):
  tok_set = None
  if path.exists(datastruct_file):
    dfile = open(datastruct_file, 'rb')
    tok_set = pickle.load(dfile)
    dfile.close()
  return tok_set

train_set = load_tok(train_tokens_file)
test_set = load_tok(test_tokens_file)

def clean_item(text):
  txt = (clean_text(re.sub("\(\d+\)", '',(str(text)[12:-26]).replace("\\n", " ")))).lower()
  mytable = txt.maketrans("-", " ", "\\,'`;()\":")
  return txt.translate(mytable)

def scrub_and_split(dataset):
  return [word_tokenize(sent.replace('..', '')) for sent in sent_tokenize(clean_item(dataset['text']))], [word_tokenize(sent.replace('..', '')) for sent in sent_tokenize(clean_item(dataset['summary']))]

def format_data(datastruct):
  data_set = {'art': [], 'sum': []}
  count = 0
  with progressbar.ProgressBar(max_value=len(datastruct)) as bar:
    for entry in datastruct:
      art, sum = scrub_and_split(entry)
      data_set['art'].append(art)
      data_set['sum'].append(sum)
      count += 1
      bar.update(count)
  return data_set

#if train_set is None:
train_set = format_data(ds_train)
#if test_set is None:
test_set = format_data(ds_test)

print(train_set['art'][:50])

#save data to file
def save(datastruct_file, dataset):
  dfile = open(datastruct_file, 'wb')
  pickle.dump(dataset, dfile)
  dfile.close()

save(train_tokens_file, train_set)
save(test_tokens_file, train_set)

def get_vocab(dataset):
    return Counter(word for subset in dataset for lst in dataset[subset] for sent in lst for word in sent).keys()

vocab_list = get_vocab(train_set)

vocab_length = len(vocab_list) + 1
print(vocab_length)

tokenizer = Tokenizer(num_words=vocab_length)
tokenizer.fit_on_texts(vocab_list)

#word_index = tokenizer.word_index
print(tokenizer.word_index["of"])

"""###Encoding"""

max_article_sentences = 50
max_article_sent_len = 40
max_summary_sentences = 10
max_summary_sent_len = 20
trunc_type='post'
padding_type='post'

def encode_text(text, summ):
  encoded_sentences = []
  encoded_text = []
  count=0
  if summ:
    sent_len = max_summary_sent_len
    max_len = max_summary_sentences
  else:
    sent_len = max_article_sent_len
    max_len = max_article_sentences
  with progressbar.ProgressBar(max_value=len(text)) as bar:
    for item in text:
      encoded_sentences.append(pad_sequences(tokenizer.texts_to_sequences(item), maxlen=sent_len, padding=padding_type, truncating=trunc_type))
      encoded_text.append(pad_sequences(encoded_sentences, maxlen=max_len, padding=padding_type, truncating=trunc_type))
      encoded_sentences.clear()
      count += 1
      bar.update(count)
  return encoded_text

def encode(data_set):
  return {'art': encode_text(data_set["art"], False), 'sum': encode_text(data_set["sum"], True)}

encoded_train = encode(train_set)
encoded_test = encode(test_set)

def finalize_data(encoded_text):
  final_art_set = []
  final_sum_set = []
  for art in encoded_text['art']:
    final_art_set.append(art[0])
  for sum in encoded_text['sum']:
    final_sum_set.append(sum[0])
  return {'art': final_art_set, 'sum': final_sum_set}

final_train_set = finalize_data(encoded_train)
final_test_set = finalize_data(encoded_test)

print(final_train_set["art"][0].shape)

"""###Decoding"""

def decode(sequence_set):
  return tokenizer.sequences_to_texts(sequence_set)

"""###Embedding

####GloVe
"""

#Creates Vectors
#taken from https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db
vectors_dict = {}
glove_file_4B = '/content/drive/My Drive/glove/glove.6B.50d.txt'
glove_file_42B = '/content/drive/My Drive/glove/glove.42B.300d.txt'
size = 400000 #4B = 400000 , 42B = 1917494
embedding_dim = 50

with open(glove_file_4B, 'r') as GloVe:
  i = 0
  #with progressbar.ProgressBar(max_value=size) as bar:
  for line in GloVe:
    values = line.split()
    word = values[0]
    vector = np.asarray(values[1:], "float32")
    vectors_dict[word] = vector
    #  bar.update(i)
    i += 1
print(i)

"""####Embedding *Matrix*"""

#Creates Embedding -> (word_index : vectors)
embedding_matrix = np.zeros((len(vocab_list)+1, embedding_dim))
i = 0
failed = 0
with progressbar.ProgressBar(max_value=vocab_length) as bar:
  for word in vocab_list:
    if word in vectors_dict:
      if word in tokenizer.word_index:
        w = tokenizer.word_index[word]
        embedding_matrix[w] = vectors_dict[word]
    else:
      failed += 1
      #print(word)
    bar.update(i)
    i += 1

print(f"Embeddings Dict size: {len(embedding_matrix)}")
print(f"{failed} words did not match the glove")
#print(embedding_matrix[tokenizer.word_index["bureau"]])
print(embedding_matrix.size)

"""#Build Model Architecture

num_of_sentences could be num of words???

https://www.researchgate.net/publication/337981958_Deep_Learning_Based_Extractive_Text_Summarization_Approaches_Datasets_and_Evaluation_Measures

Builds Model
"""

#Initializers

p_W, p_U, p_emb, p_dense, weight_decay = 0, 0, 0, 0, 0
rnn_layers, rnn_size = 3, 512
from keras.regularizers import l2
regularizer = l2(weight_decay) if weight_decay else None

#Build Model
print(embedding_matrix.shape)
vocab_size, embedding_size = embedding_matrix.shape

model = keras.Sequential()

model.add(layers.Embedding(vocab_size, embedding_size,
                    embeddings_regularizer=regularizer, weights=[embedding_matrix], mask_zero=True,
                    name='embedding_1'))

for i in range(rnn_layers):
    lstm = layers.LSTM(rnn_size, return_sequences=True, # batch_norm=batch_norm,
                kernel_regularizer=regularizer, recurrent_regularizer=regularizer,
                bias_regularizer=regularizer, dropout=p_W, recurrent_dropout=p_U,
                name='lstm_%d'%(i+1)
                  )
    model.add(lstm)
    print(lstm.input_shape)
    model.add(layers.Dropout(p_dense, name='dropout_%d'%(i+1)))

model.add(layers.TimeDistributed(layers.Dense(max_summary_sentences,
                                kernel_regularizer=regularizer, bias_regularizer=regularizer,
                                name = 'timedistributed_1')))

model.add(layers.Activation('softmax', name='activation_1'))

from keras.optimizers import RMSprop, Adam

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
model.summary()

#sets the train and test as clean text. Where x = article and y = summary
x_train = np.array(final_train_set["art"])
y_train = np.array(final_train_set["sum"])
print(x_train.shape)
print(y_train.shape)
x_test = np.array(final_test_set["art"])
y_test = np.array(final_test_set["sum"])

model.fit(x_train, y_train, epochs=5, verbose=1)
loss, accuracy = model.evaluate(x_test, y_test, verbose=0)
print('Accuracy: %f' % (accuracy*100))

model = keras.Sequential()
embedding_layer = layers.Embedding(vocab_length, embedding_dim, weights=[embedding_matrix], input_length=max_article_length, trainable=False)
model.add(embedding_layer)
model.add(layers.Flatten())
model.add(layers.Dense(vocab_length*0.02, activation='sigmoid'))
model.add(layers.Dense(vocab_length*0.5, activation='sigmoid'))
model.add(layers.Dense(max_summary_length, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
model.summary()