# -*- coding: utf-8 -*-
"""FLAW.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1as8FpftkzIutE3QbjUpa6rfd_fmD_ggY

#IMPORTS
"""

#Keras and TensorFlow
import keras
from keras import layers
import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
#Progress Bars
import time
import progressbar
#Encoding
from tensorflow.keras.preprocessing.sequence import pad_sequences
import sys
#Tokenize
import collections
from collections import Counter
import itertools
from itertools import chain
import string
import nltk as nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
nltk.download('punkt')

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('To enable a high-RAM runtime, select the Runtime > "Change runtime type"')
  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')
  print('re-execute this cell.')
else:
  print('You are using a high-RAM runtime!')

"""#Build Dataset

##Download Dataset
"""

ds, info = tfds.load("billsum", with_info=True)
ds_ca_test, ds_test, ds_train = ds["ca_test"], ds["test"], ds["train"] 
print(info)
print(ds_train)

ds_numpy = tfds.as_numpy(ds_train);

#for summ in ds_numpy:
 # print(summ)

###PRINTS EVERY SUMMARY IN THE DS
#for ins in ds_train:
 # summary_i = ins["summary"]
  #print(summary_i)



#FeaturesDict({
 #   'summary': Text(shape=(), dtype=tf.string),
  #  'text': Text(shape=(), dtype=tf.string),
   # 'title': Text(shape=(), dtype=tf.string),
#})

"""##Dataset

####Clean DS Code from DS creator
"""

import os
import pandas as pd
import pickle
import re
import unicodedata

def normalize_input(text):
  return unicodedata.normalize('NFD', text).encode('ascii', 'ignore')
  
def replace_semicolon(text, threshold=10):
    '''
    Get rid of semicolons.
    First split text into fragments between the semicolons. If the fragment 
    is longer than the threshold, turn the semicolon into a period. O.w treat
    it as a comma.
    Returns new text
    '''
    new_text = ""
    for subset in re.split(';', text):
        subset = subset.strip() # Clear off spaces
        # Check word count
        if len(subset.split()) > threshold:
            # Turn first char into uppercase
            new_text += ". " + subset[0].upper() + subset[1:]
        else:
            # Just append with a comma 
            new_text += ", " + subset

    return new_text

USC_re = re.compile('[Uu]\.*[Ss]\.*[Cc]\.]+')
PAREN_re = re.compile('\([^(]+\ [^\(]+\)')
BAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('"#%&\*\+/<=>@[\]^{|}~_'), re.UNICODE)
BULLET_RE = re.compile('\n[\ \t]*`*\([a-zA-Z0-9]*\)')
DASH_RE = re.compile('--+')
WHITESPACE_RE = re.compile('\s+')
EMPTY_SENT_RE = re.compile('[,\.]\ *[\.,]')
FIX_START_RE = re.compile('^[^A-Za-z]*')
FIX_PERIOD = re.compile('\.([A-Za-z])')
SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\.|\nSEC\.* [0-9]{1,2}\.|Sec\.* [0-9]{1,2}\.')

FIX_PERIOD = re.compile('\.([A-Za-z])')

SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\.|\nSEC\.* [0-9]{1,2}\.|Sec\.* [0-9]{1,2}\.')

def clean_text(text):
    """
    Borrowed from the FNDS text processing with additional logic added in.
    Note: we do not take care of token breaking - assume SPACY's tokenizer
    will handle this for us.
    """

    # Indicate section headers, we need them for features
    text = SECTION_HEADER_RE.sub('SECTION-HEADER', text)
    # For simplicity later, remove '.' from most common acronym
    text = text.replace("U.S.", "US")
    text = text.replace('SEC.', 'Section')
    text = text.replace('Sec.', 'Section')
    text = USC_re.sub('USC', text)

    # Remove parantheticals because they are almost always references to laws 
    # We could add a special tag, but we just remove for now
    # Note we dont get rid of nested parens because that is a complex re
    #text = PAREN_re.sub('LAWREF', text)
    text = PAREN_re.sub('', text)
    

    # Get rid of enums as bullets or ` as bullets
    text = BULLET_RE.sub(' ',text)
    
    # remove &nbsp
    text = text.replace('&nbsp', '')


    # Clean html 
    text = text.replace('&lt;all&gt;', '')

    # Remove annoying punctuation, that's not relevant
    text = BAD_PUNCT_RE.sub('', text)

    # Get rid of long sequences of dashes - these are formating
    text = DASH_RE.sub( ' ', text)

    # removing newlines, tabs, and extra spaces.
    text = WHITESPACE_RE.sub(' ', text)
    
    # If we ended up with "empty" sentences - get rid of them.
    text = EMPTY_SENT_RE.sub('.', text)
    
    # Attempt to create sentences from bullets 
    text = replace_semicolon(text)
    
    # Fix weird period issues + start of text weirdness
    #text = re.sub('\.(?=[A-Z])', '  . ', text)
    # Get rid of anything thats not a word from the start of the text
    text = FIX_START_RE.sub( '', text)
    # Sometimes periods get formatted weird, make sure there is a space between periods and start of sent   
    text = FIX_PERIOD.sub(". \g<1>", text)

    # Fix quotes
    text = text.replace('``', '"')
    text = text.replace('\'\'', '"')
    text = str(normalize_input(text))

    # Add special punct back in
    text = text.replace('SECTION-HEADER', '')
    text = text.replace('SHORT TITLE.', '')

    text = re.sub('Section [0-9]{1,2}\.', '', text)
    text = re.sub("(\w)quot(\s|\.|$)", r"\1 ", text)

    return text

"""###Tokenize"""

train_tokens_file = '/content/drive/My Drive/glove/train_tokens.pkl'
test_tokens_file =  '/content/drive/My Drive/glove/test_tokens.pkl'
import os.path
from os import path
#load object from file
def load_tok(datastruct_file):
  tok_set = None
  if path.exists(datastruct_file):
    dfile = open(datastruct_file, 'rb')
    tok_set = pickle.load(dfile)
    dfile.close()
  return tok_set

train_set = load_tok(train_tokens_file)
test_set = load_tok(test_tokens_file)

def clean_item(text):
  txt = (clean_text(re.sub("\(\d\)", '',(str(text)[12:-26]).replace("\\n", " ")))).lower()
  mytable = txt.maketrans(".-", "  ", ",'`;)\":")
  return txt.translate(mytable)

def clean_and_tokenize(datastruct):
  return word_tokenize(clean_item(datastruct['summary'])), word_tokenize(clean_item(datastruct['text']))

def format_data(datastruct):
  data_set = {'x': [], 'y': []}
  count = 0
  with progressbar.ProgressBar(max_value=len(datastruct)) as bar:
    for entry in datastruct:
      art, sum = clean_and_tokenize(entry)
      data_set['x'].append(art)
      data_set['y'].append(sum)
      count += 1
      bar.update(count)
  return data_set

#save data to file
def save(datastruct_file, dataset):
  dfile = open(datastruct_file, 'wb')
  pickle.dump(dataset, dfile)
  dfile.close()

if True:
  train_set = format_data(ds_train)
  test_set = format_data(ds_test)
  save(train_tokens_file, train_set)
  save(test_tokens_file, train_set)

def get_vocab(dataset):
    return Counter(item for subset in dataset for lst in dataset[subset] for item in lst).keys()

vocab_list = get_vocab(train_set)

#vocab_length = len(vocab_list) + 1
#print(vocab_length)

#tokenizer = Tokenizer(num_words=vocab_length)
#tokenizer.fit_on_texts(vocab_list)

#word_index = tokenizer.word_index
#print(tokenizer.word_index["of"])

"""###Old Encoding

old implementation
"""

max_article_length = 2000
max_summary_length = 2000
embedding_dim = 50
trunc_type='post'
padding_type='post'

def encode(data_set):
  art_sequences = tokenizer.texts_to_sequences(data_set['x'])
  art_padded = pad_sequences(art_sequences, maxlen=max_article_length, padding=padding_type, truncating=trunc_type)
  sum_sequences = tokenizer.texts_to_sequences(data_set['y'])
  sum_padded = pad_sequences(sum_sequences, maxlen=max_summary_length, padding=padding_type, truncating=trunc_type)

  return {'x': art_padded, 'y': sum_padded}

training_padded = encode(train_set)
testing_padded = encode(test_set)

del train_set
del test_set

"""###Decoding"""

def decode(sequence_set):
  return tokenizer.sequences_to_texts(sequence_set)

"""###Embedding

####GloVe
"""

#Creates Vectors
#taken from https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db
vectors_dict = {}
glove_file_4B = '/content/drive/My Drive/glove/glove.6B.50d.txt'
glove_file_42B = '/content/drive/My Drive/glove/glove.42B.300d.txt'
size = 400000 #4B = 400000 , 42B = 1917494
embedding_dim = 50

with open(glove_file_4B, 'r') as GloVe:
  i = 0
  #with progressbar.ProgressBar(max_value=size) as bar:
  for line in GloVe:
    values = line.split()
    word = values[0]
    vector = np.asarray(values[1:], "float32")
    vectors_dict[word] = vector
    #  bar.update(i)
    i += 1
print(i)

"""####Embedding *Matrix*"""

#Creates Embedding -> (word_index : vectors)
i = 0
failed = 0
embedding_dictionary = {}

with progressbar.ProgressBar(max_value=len(vocab_list)) as bar:
  for word in vocab_list:
    if word in vectors_dict:
      embedding_dictionary[word] = vectors_dict[word].astype('float32')
    else:
      failed += 1
      #print(word)
    bar.update(i)
    i += 1

word_count = len(embedding_dictionary)+2
print(f"Embeddings Dict size: {word_count}")
print(f"{failed} words did not match the glove")
#print(embedding_matrix[tokenizer.word_index["bureau"]])
print(sys.getsizeof(embedding_dictionary))

tokenizer = Tokenizer(num_words=word_count, oov_token='OOV')
tokenizer.fit_on_texts(embedding_dictionary.keys())

embedding_matrix = np.zeros((word_count, embedding_dim), dtype='float32')

for w in tokenizer.word_index:
  if w in embedding_dictionary:
    embedding_matrix[tokenizer.word_index[w]] = embedding_dictionary[w]

del embedding_dictionary

"""##New Encoding"""

max_article_length = 2000
max_summary_length = 2000
trunc_type='post'
padding_type='post'

def encode(data_set):
  art_sequences = tokenizer.texts_to_sequences(data_set['x'])
  art_padded = pad_sequences(art_sequences, maxlen=max_article_length, padding=padding_type, truncating=trunc_type)
  sum_sequences = tokenizer.texts_to_sequences(data_set['y'])
  sum_padded = pad_sequences(sum_sequences, maxlen=max_summary_length, padding=padding_type, truncating=trunc_type)
  return {'x': art_padded, 'y': sum_padded}

training_padded = encode(train_set)
testing_padded = encode(test_set)

del train_set
del test_set

"""#Build Model Architecture

num_of_sentences could be num of words???

https://www.researchgate.net/publication/337981958_Deep_Learning_Based_Extractive_Text_Summarization_Approaches_Datasets_and_Evaluation_Measures

##Initialize
"""

#Initializers

p_W, p_U, p_emb, p_dense, weight_decay = 0, 0, 0, 0.1, 0
rnn_layers, rnn_size = 3, 512
from keras.regularizers import l2
regularizer = l2(weight_decay) if weight_decay else None

def find_closest(embeddings, embedding_dict):
  divisors = np.matmul(np.linalg.norm(embeddings, axis=1).reshape(len(embeddings), 1), np.linalg.norm(embedding_dict, axis=1).reshape(1, len(embedding_dict)))+np.float32(0.00001)
  numerators = np.tensordot(embeddings, embedding_dict, axes=(1,1))
  return np.argmax(numerators/divisors, axis=1)


class Decode(keras.layers.Layer):
    def init(self, embedding_dict):
        super(Decode, self).init()
        self.embedding_dict = embedding_dict

    def compute_mask(self, inputs, mask=None):
        return None

    def call(self, inputs):
        emb_distances = tf.matmul(tf.nn.l2_normalize(inputs, axis=1), tf.nn.l2_normalize(self.embedding_dict, axis=1), transpose_b=True)
        return tf.argmax(emb_distances, axis=0)

"""##Build Model"""

#Build Model
print(embedding_matrix.shape)
vocab_size, embedding_size = embedding_matrix.shape

model = keras.Sequential()
model.add(layers.Embedding(vocab_size, embedding_size, input_length=max_article_length,
                    embeddings_regularizer=regularizer, weights=[embedding_matrix], mask_zero=True))

for i in range(rnn_layers):
    lstm = layers.LSTM(rnn_size, return_sequences=True)
    model.add(lstm)
    model.add(layers.Dropout(p_dense))



model.add(layers.TimeDistributed(layers.Dense(embedding_dim)))
#model.add(Decode(embedding_matrix))
#model.add(layers.Reshape((max_summary_length, embedding_dim)))
model.add(layers.Activation('softmax'))

from keras.optimizers import RMSprop, Adam

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
model.summary()

"""#Build Input

def find_closest(embeddings, embedding_dict):
  emb_distances = tf.matmul(tf.nn.l2_normalize(embeddings, axis=1), tf.nn.l2_normalize(embedding_dict, axis = 1), transpose_b=True)
  return emb_distances
#sets the train and test as clean text. Where x = article and y = summary
def embed_sum(sums):
  embedding_mat = []
  for sum in sums:
    for word in sum:
      embedding_mat.append(embedding_matrix[word])
  return embedding_mat

emb_train = embed_sum(training_padded['x'])
emb_test = embed_sum(testing_padded['x'])
  #return [[embedding_matrix[word] for word in sum] for sum in sums]
x_train = np.array(training_padded["x"])
y_train = np.array(emb_train)
del training_padded
del emb_train
x_test = np.array(testing_padded["x"])
y_test = np.array(emb_test)
del testing_padded
del emb_test
"""

num_predictions = 3
test_set = np.asarray(testing_padded['x'][:3])

predictions = model.predict(test_set)


print(len(test_set[0]))

test = embedding_matrix[test_set]

print("predictions shape:", predictions.shape)
print("embedding matrix shape:", embedding_matrix.shape)
print(len(test))
test = [find_closest(item, embedding_matrix) for item in test]
test_predict = [find_closest(predict, embedding_matrix) for predict in predictions]
print(tokenizer.sequences_to_texts(test)[0])
print(tokenizer.sequences_to_texts(test_predict)[0])

"""##Train"""

model_path =  '/content/drive/My Drive/keras/models/test.keras'

model = keras.models.load_model(model_path)

def generator(dataset, batch_size, steps):
  idx=1
  while True: 
    start = (idx - 1) * batch_size
    yield (np.array(dataset['x'][start:start+batch_size]), np.array(embedding_matrix[dataset['y'][start:start+batch_size]]))
    if idx<steps:
      idx+=1
    else:
      idx=1

num_epochs = 10
batch_size = 4
steps_per_epoch = len(training_padded['x'])/batch_size
validation_steps = len(testing_padded['x'])/batch_size

train_gen = generator(training_padded, batch_size, steps_per_epoch)
test_gen = generator(testing_padded, batch_size, validation_steps)

model.fit(train_gen, epochs=num_epochs, steps_per_epoch=steps_per_epoch, 
  verbose=1, validation_data=test_gen, validation_steps=validation_steps)

keras.models.save_model(model, model_path)

print(np.shape(x_train))
print(np.shape(y_train))
model.fit(x_train, y_train, epochs=1, verbose=1)
loss, accuracy = model.evaluate(x_test, y_test,batch_size=n_batch, verbose=0)
print('Accuracy: %f' % (accuracy*100))

"""##OLD"""

model = keras.Sequential()
embedding_layer = layers.Embedding(vocab_length, embedding_dim, weights=[embedding_matrix], input_length=max_article_length, trainable=False)
model.add(embedding_layer)
model.add(layers.Flatten())
model.add(layers.Dense(vocab_length*0.02, activation='sigmoid'))
model.add(layers.Dense(vocab_length*0.5, activation='sigmoid'))
model.add(layers.Dense(max_summary_length, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
model.summary()