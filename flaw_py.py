# -*- coding: utf-8 -*-
"""FLAW.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1as8FpftkzIutE3QbjUpa6rfd_fmD_ggY

#IMPORTS
"""

import keras
from keras import layers
import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np


print(np.version)

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive

"""#Build Dataset

##Download Dataset
"""

ds, info = tfds.load("billsum", with_info=True)
ds_ca_test, ds_test, ds_train = ds["ca_test"], ds["test"], ds["train"] 
print(info)
print(ds_train)

ds_numpy = tfds.as_numpy(ds_train);

#for summ in ds_numpy:
 # print(summ)

###PRINTS EVERY SUMMARY IN THE DS
#for ins in ds_train:
 # summary_i = ins["summary"]
  #print(summary_i)



#FeaturesDict({
 #   'summary': Text(shape=(), dtype=tf.string),
  #  'text': Text(shape=(), dtype=tf.string),
   # 'title': Text(shape=(), dtype=tf.string),
#})

"""##Clean Dataset"""

import os
import pandas as pd
import pickle
import re

def replace_semicolon(text, threshold=10):
    '''
    Get rid of semicolons.
    First split text into fragments between the semicolons. If the fragment 
    is longer than the threshold, turn the semicolon into a period. O.w treat
    it as a comma.
    Returns new text
    '''
    new_text = ""
    for subset in re.split(';', text):
        subset = subset.strip() # Clear off spaces
        # Check word count
        if len(subset.split()) > threshold:
            # Turn first char into uppercase
            new_text += ". " + subset[0].upper() + subset[1:]
        else:
            # Just append with a comma 
            new_text += ", " + subset

    return new_text

USC_re = re.compile('[Uu]\.*[Ss]\.*[Cc]\.]+')
PAREN_re = re.compile('\([^(]+\ [^\(]+\)')
BAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('"#%&\*\+/<=>@[\]^{|}~_'), re.UNICODE)
BULLET_RE = re.compile('\n[\ \t]*`*\([a-zA-Z0-9]*\)')
DASH_RE = re.compile('--+')
WHITESPACE_RE = re.compile('\s+')
EMPTY_SENT_RE = re.compile('[,\.]\ *[\.,]')
FIX_START_RE = re.compile('^[^A-Za-z]*')
FIX_PERIOD = re.compile('\.([A-Za-z])')
SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\.|\nSEC\.* [0-9]{1,2}\.|Sec\.* [0-9]{1,2}\.')

FIX_PERIOD = re.compile('\.([A-Za-z])')

SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\.|\nSEC\.* [0-9]{1,2}\.|Sec\.* [0-9]{1,2}\.')

def clean_text(text):
    """
    Borrowed from the FNDS text processing with additional logic added in.
    Note: we do not take care of token breaking - assume SPACY's tokenizer
    will handle this for us.
    """

    # Indicate section headers, we need them for features
    text = SECTION_HEADER_RE.sub('SECTION-HEADER', text)
    # For simplicity later, remove '.' from most common acronym
    text = text.replace("U.S.", "US")
    text = text.replace('SEC.', 'Section')
    text = text.replace('Sec.', 'Section')
    text = USC_re.sub('USC', text)

    # Remove parantheticals because they are almost always references to laws 
    # We could add a special tag, but we just remove for now
    # Note we dont get rid of nested parens because that is a complex re
    #text = PAREN_re.sub('LAWREF', text)
    text = PAREN_re.sub('', text)
    

    # Get rid of enums as bullets or ` as bullets
    text = BULLET_RE.sub(' ',text)
    
    # remove &nbsp
    text = text.replace('&nbsp', '')

    # Clean html 
    text = text.replace('&lt;all&gt;', '')

    # Remove annoying punctuation, that's not relevant
    text = BAD_PUNCT_RE.sub('', text)

    # Get rid of long sequences of dashes - these are formating
    text = DASH_RE.sub( ' ', text)

    # removing newlines, tabs, and extra spaces.
    text = WHITESPACE_RE.sub(' ', text)
    
    # If we ended up with "empty" sentences - get rid of them.
    text = EMPTY_SENT_RE.sub('.', text)
    
    # Attempt to create sentences from bullets 
    text = replace_semicolon(text)
    
    # Fix weird period issues + start of text weirdness
    #text = re.sub('\.(?=[A-Z])', '  . ', text)
    # Get rid of anything thats not a word from the start of the text
    text = FIX_START_RE.sub( '', text)
    # Sometimes periods get formatted weird, make sure there is a space between periods and start of sent   
    text = FIX_PERIOD.sub(". \g<1>", text)

    # Fix quotes
    text = text.replace('``', '"')
    text = text.replace('\'\'', '"')

    # Add special punct back in
    text = text.replace('SECTION-HEADER', '<SECTION-HEADER>')

    return text

"""###Tokenize"""

import collections
from collections import Counter
import itertools
from itertools import chain
import string
import nltk as nltk
from nltk.tokenize import sent_tokenize, word_tokenize

nltk.download('punkt')

def clean(text):
  txt = (clean_text(re.sub("(\([^)]*\))", '',(str(text)[12:-26]).replace("\\n", " ")))).lower()
  mytable = txt.maketrans(".-", "  ", ",'`")
  return txt.translate(mytable)


lst = []
count = 0
print("Cleaning")
for entry in ds_train:
  lst.extend(word_tokenize(clean(entry['summary'])+" "+clean(entry['text'])+" "+clean(entry['title'])))
  #lst.append(re.sub("(\([^)]*\))", '',(str(entry['summary'])[12:-26]).replace("\\n", " ")))
  count += 1
  if count % 1000 == 0:
    print(count)

print(lst[:50])

def get_vocab(lst):
    vocabcount = Counter(lst)
    vocab = map(lambda x: x[0], sorted(vocabcount.items(), key=lambda x: -x[1]))
    return vocab, vocabcount

vocab, vocabcount = get_vocab(lst)

print(len(vocabcount))

"""###Vectorize

####GloVe
"""

#Creates Vectors
#taken from https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db
vectors_dict = {}
with open('/content/drive/My Drive/glove/glove.42B.300d.txt', 'r') as GloVe:
  i = 0
  for line in GloVe:
    values = line.split()
    word = values[0]
    vector = np.asarray(values[1:], "float32")
    vectors_dict[word] = vector

print(len(vectors_dict))

#Creates Embedding
embeddings_dict = {}
for word in vocabcount:
  sum = 0
  if(word in vectors_dict):
    inst = vectors_dict[word]
    embeddings_dict[word] = map(float,inst[1:])
  else:
    print(word)

print(len(embeddings_dict))

"""#Build Model Architecture

num_of_sentences could be num of words???

https://www.researchgate.net/publication/337981958_Deep_Learning_Based_Extractive_Text_Summarization_Approaches_Datasets_and_Evaluation_Measures

Builds Model
"""

#still needs work
#def build_Model(embeddings):
x_num_of_words = len(embeddings_dict) #???
y_num_of_words = len(embeddings_dict) #?

layerMultiplier = 0.01

inputs = keras.Input(shape=(x_num_of_words,))

inputs.shape
inputs.dtype

dense = layers.Dense(x_num_of_words * layerMultiplier, activation='relu')
x = dense(inputs)

x = layers.Dense(x_num_of_words * layerMultiplier, activation='relu')(x)
output = layers.Dense(y_num_of_words)(x)

model = keras.Model(inputs=inputs, outputs=output, name='First_Model')

model.summary()
 # return model

  #lebuild_Model(vectors_dict)