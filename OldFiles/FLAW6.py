# -*- coding: utf-8 -*-
"""FLAW.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1as8FpftkzIutE3QbjUpa6rfd_fmD_ggY

#IMPORTS
"""

#Keras and TensorFlow
import keras
from keras import layers
import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
#Progress Bars
import time
import progressbar
#Encoding
from tensorflow.keras.preprocessing.sequence import pad_sequences
#Tokenize
import collections
from collections import Counter
import itertools
from itertools import chain
import string
import nltk as nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
nltk.download('punkt')

# Commented out IPython magic to ensure Python compatibility.
#from google.colab import drive
#drive.mount('/content/drive')
# %cd /content/drive

"""#Build Dataset

##Download Dataset
"""

ds, info = tfds.load("billsum", with_info=True)
ds_ca_test, ds_test, ds_train = ds["ca_test"], ds["test"], ds["train"] 
print(info)
print(ds_train)

ds_numpy = tfds.as_numpy(ds_train);

#for summ in ds_numpy:
 # print(summ)

###PRINTS EVERY SUMMARY IN THE DS
#for ins in ds_train:
 # summary_i = ins["summary"]
  #print(summary_i)



#FeaturesDict({
 #   'summary': Text(shape=(), dtype=tf.string),
  #  'text': Text(shape=(), dtype=tf.string),
   # 'title': Text(shape=(), dtype=tf.string),
#})

"""##Dataset

####Clean DS Code from DS creator
"""

import os
import pandas as pd
import pickle
import re
import unicodedata

def normalize_input(text):
  return unicodedata.normalize('NFD', text).encode('ascii', 'ignore')
  
def replace_semicolon(text, threshold=10):
    '''
    Get rid of semicolons.
    First split text into fragments between the semicolons. If the fragment 
    is longer than the threshold, turn the semicolon into a period. O.w treat
    it as a comma.
    Returns new text
    '''
    new_text = ""
    for subset in re.split(';', text):
        subset = subset.strip() # Clear off spaces
        # Check word count
        if len(subset.split()) > threshold:
            # Turn first char into uppercase
            new_text += ". " + subset[0].upper() + subset[1:]
        else:
            # Just append with a comma 
            new_text += ", " + subset

    return new_text

USC_re = re.compile('[Uu]\.*[Ss]\.*[Cc]\.]+')
PAREN_re = re.compile('\([^(]+\ [^\(]+\)')
BAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('"#%&\*\+/<=>@[\]^{|}~_'), re.UNICODE)
BULLET_RE = re.compile('\n[\ \t]*`*\([a-zA-Z0-9]*\)')
DASH_RE = re.compile('--+')
WHITESPACE_RE = re.compile('\s+')
EMPTY_SENT_RE = re.compile('[,\.]\ *[\.,]')
FIX_START_RE = re.compile('^[^A-Za-z]*')
FIX_PERIOD = re.compile('\.([A-Za-z])')
SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\.|\nSEC\.* [0-9]{1,2}\.|Sec\.* [0-9]{1,2}\.')

FIX_PERIOD = re.compile('\.([A-Za-z])')

SECTION_HEADER_RE = re.compile('SECTION [0-9]{1,2}\.|\nSEC\.* [0-9]{1,2}\.|Sec\.* [0-9]{1,2}\.')

def clean_text(text):
    """
    Borrowed from the FNDS text processing with additional logic added in.
    Note: we do not take care of token breaking - assume SPACY's tokenizer
    will handle this for us.
    """

    # Indicate section headers, we need them for features
    text = SECTION_HEADER_RE.sub('SECTION-HEADER', text)
    # For simplicity later, remove '.' from most common acronym
    text = text.replace("U.S.", "US")
    text = text.replace('SEC.', 'Section')
    text = text.replace('Sec.', 'Section')
    text = USC_re.sub('USC', text)

    # Remove parantheticals because they are almost always references to laws 
    # We could add a special tag, but we just remove for now
    # Note we dont get rid of nested parens because that is a complex re
    #text = PAREN_re.sub('LAWREF', text)
    text = PAREN_re.sub('', text)
    

    # Get rid of enums as bullets or ` as bullets
    text = BULLET_RE.sub(' ',text)
    
    # remove &nbsp
    text = text.replace('&nbsp', '')


    # Clean html 
    text = text.replace('&lt;all&gt;', '')

    # Remove annoying punctuation, that's not relevant
    text = BAD_PUNCT_RE.sub('', text)

    # Get rid of long sequences of dashes - these are formating
    text = DASH_RE.sub( ' ', text)

    # removing newlines, tabs, and extra spaces.
    text = WHITESPACE_RE.sub(' ', text)
    
    # If we ended up with "empty" sentences - get rid of them.
    text = EMPTY_SENT_RE.sub('.', text)
    
    # Attempt to create sentences from bullets 
    text = replace_semicolon(text)
    
    # Fix weird period issues + start of text weirdness
    #text = re.sub('\.(?=[A-Z])', '  . ', text)
    # Get rid of anything thats not a word from the start of the text
    text = FIX_START_RE.sub( '', text)
    # Sometimes periods get formatted weird, make sure there is a space between periods and start of sent   
    text = FIX_PERIOD.sub(". \g<1>", text)

    # Fix quotes
    text = text.replace('``', '"')
    text = text.replace('\'\'', '"')
    text = str(normalize_input(text))

    # Add special punct back in
    text = text.replace('SECTION-HEADER', '')
    text = text.replace('SHORT TITLE.', '')

    text = re.sub('Section [0-9]{1,2}\.', '', text)
    text = re.sub("(\w)quot(\s|\.|$)", r"\1 ", text)

    return text

"""###Tokenize"""

train_tokens_file = '/content/drive/My Drive/glove/train_tokens.pkl'
test_tokens_file =  '/content/drive/My Drive/glove/test_tokens.pkl'
import os.path
from os import path
#load object from file
def load_tok(datastruct_file):
  tok_set = None
  if path.exists(datastruct_file):
    dfile = open(datastruct_file, 'rb')
    tok_set = pickle.load(dfile)
    dfile.close()
  return tok_set

#train_set = load_tok(train_tokens_file)
#test_set = load_tok(test_tokens_file)

def clean_item(text):
  txt = (clean_text(re.sub("\(\d\)", '',(str(text)[12:-26]).replace("\\n", " ")))).lower()
  mytable = txt.maketrans(".-", "  ", ",'`;)\":")
  return txt.translate(mytable)

def clean_and_tokenize(datastruct):
  return word_tokenize(clean_item(datastruct['summary'])), word_tokenize(clean_item(datastruct['text']))

def format_data(datastruct):
  data_set = {'art': [], 'sum': []}
  count = 0
  with progressbar.ProgressBar(max_value=len(datastruct)) as bar:
    for entry in datastruct:
      art, sum = clean_and_tokenize(entry)
      data_set['art'].append(art)
      data_set['sum'].append(sum)
      count += 1
      bar.update(count)
  return data_set

#save data to file
def save(datastruct_file, dataset):
  dfile = open(datastruct_file, 'wb')
  pickle.dump(dataset, dfile)
  dfile.close()

if True: #train_set is None:
  train_set = format_data(ds_train)
  test_set = format_data(ds_test)
  save(train_tokens_file, train_set)
  save(test_tokens_file, train_set)

def get_vocab(dataset):
    return Counter(item for subset in dataset for lst in dataset[subset] for item in lst).keys()

vocab_list = get_vocab(train_set)

vocab_length = len(vocab_list) + 1
print(vocab_length)

tokenizer = Tokenizer(num_words=vocab_length)
tokenizer.fit_on_texts(vocab_list)

#word_index = tokenizer.word_index
print(tokenizer.word_index["of"])

"""###Encoding"""

max_article_length = 2000
max_summary_length = 300
embedding_dim = 50
trunc_type='post'
padding_type='post'

def encode(data_set):
  art_sequences = tokenizer.texts_to_sequences(data_set['art'])
  art_padded = pad_sequences(art_sequences, maxlen=max_article_length, padding=padding_type, truncating=trunc_type)
  sum_sequences = tokenizer.texts_to_sequences(data_set['sum'])
  sum_padded = pad_sequences(sum_sequences, maxlen=max_summary_length, padding=padding_type, truncating=trunc_type)

  return {'art': art_padded, 'sum': sum_padded}

training_padded = encode(train_set)
testing_padded = encode(test_set)

"""###Decoding"""

def decode(sequence_set):
  return tokenizer.sequences_to_texts(sequence_set)

"""###Embedding

####GloVe
"""

#Creates Vectors
#taken from https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db
vectors_dict = {}
glove_file_4B = '/content/drive/My Drive/glove/glove.6B.50d.txt'
glove_file_42B = '/content/drive/My Drive/glove/glove.42B.300d.txt'
size = 400000 #4B = 400000 , 42B = 1917494
embedding_dim = 50

with open(glove_file_4B, 'r') as GloVe:
  i = 0
  #with progressbar.ProgressBar(max_value=size) as bar:
  for line in GloVe:
    values = line.split()
    word = values[0]
    vector = np.asarray(values[1:], "float32")
    vectors_dict[word] = vector
    #  bar.update(i)
    i += 1
print(i)

"""####Embedding *Matrix*"""

#Creates Embedding -> (word_index : vectors)
embedding_matrix = np.zeros((len(vocab_list)+1, embedding_dim))
i = 0
failed = 0
with progressbar.ProgressBar(max_value=vocab_length) as bar:
  for word in vocab_list:
    if word in vectors_dict:
      if word in tokenizer.word_index:
        w = tokenizer.word_index[word]
        embedding_matrix[w] = vectors_dict[word]
    else:
      failed += 1
      #print(word)
    bar.update(i)
    i += 1

print(f"Embeddings Dict size: {len(embedding_matrix)}")
print(f"{failed} words did not match the glove")
#print(embedding_matrix[tokenizer.word_index["bureau"]])
print(embedding_matrix.size)

"""#Build Model Architecture

num_of_sentences could be num of words???

https://www.researchgate.net/publication/337981958_Deep_Learning_Based_Extractive_Text_Summarization_Approaches_Datasets_and_Evaluation_Measures

Builds Model
"""

#Initializers

p_W, p_U, p_emb, p_dense, weight_decay = 0, 0, 0, 0, 0
rnn_layers, rnn_size = 3, 512
from keras.regularizers import l2
regularizer = l2(weight_decay) if weight_decay else None

#Build Model
print(embedding_matrix.shape)
vocab_size, embedding_size = embedding_matrix.shape

model = keras.Sequential()
model.add(layers.Embedding(vocab_size, embedding_size,
                    embeddings_regularizer=regularizer, weights=[embedding_matrix], mask_zero=True))

for i in range(rnn_layers):
    lstm = layers.LSTM(rnn_size, return_sequences=True)
    model.add(lstm)
    model.add(layers.Dropout(p_dense))

model.add(layers.TimeDistributed(layers.Dense(embedding_dim)))
model.add(layers.Reshape((max_summary_length, embedding_dim)))
model.add(layers.Activation('softmax'))

from keras.optimizers import RMSprop, Adam

model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
model.summary()

#sets the train and test as clean text. Where x = article and y = summary
def embed_sum(sums):
  return [[embedding_matrix[word] for word in sum] for sum in sums]

x_train = np.array(training_padded["art"])
y_train = np.array(embed_sum(training_padded['sum']))

x_test = np.array(testing_padded["art"])
y_test = np.array(embed_sum(testing_padded['sum']))

print(np.shape(x_train))
print(np.shape(y_train))
model.fit(x_train, y_train, epochs=1, verbose=1)
loss, accuracy = model.evaluate(x_test, y_test, verbose=0)
print('Accuracy: %f' % (accuracy*100))

model = keras.Sequential()
embedding_layer = layers.Embedding(vocab_length, embedding_dim, weights=[embedding_matrix], input_length=max_article_length, trainable=False)
model.add(embedding_layer)
model.add(layers.Flatten())
model.add(layers.Dense(vocab_length*0.02, activation='sigmoid'))
model.add(layers.Dense(vocab_length*0.5, activation='sigmoid'))
model.add(layers.Dense(max_summary_length, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
model.summary()
